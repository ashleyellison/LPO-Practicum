---
title: Data cleaning
author: LPO 9951 | Fall 2016
output: github_document
---

```{r, echo = FALSE, message = FALSE}
require(knitr)
source('../../r/stataknitrhelper.r')
opts_chunk$set(echo = FALSE, message = FALSE, comment = NA)
```

```{r, engine = 'bash'}
## run accompanying .do file to get log file for parsing
#stata -b -q do ../do/lecture9_cleaning.do
## convert plots used in this file to png
pdir=
glist=(hist_read_scr
       sc_str_str_two
       sc_str_str_three
       hist_str
       sc_inc_meal
       box_str
      )
for i in ${glist[@]};
do
convert -density 150 -flatten $pdir$i.eps $pdir$i.png;
done
```

```{r}
## save log file in object
lf <- 'cleaning.log'
```

<br>

#### PURPOSE

Data cleaning is a term which refers to a variety of highly applied
activities that are intended to ensure that the data are not, in some
way, incorrect.  

A "clean" dataset is one in which every variable is labeled
appropriately, and organized in a way that will make sense to a
user. If missing data exists, there is a unified approach to dealing
with it (i.e., every missing data point is coded in the same way). The
data have been checked for consistency using a variety of more or less
common sense approaches. It's important to note from the outset that
data-cleaning is not a specific data analytic approach so much as an
attitude about the data, combined with some specific techniques. Here
are some, but only some, of those techniques.

We'll be using a deliberately "dirtied up" dataset. This is based on
the `caschools` data, which contains data on all of the school districts
in the state of California.  

<br>

## Variable labels

Not everyone you work with will be as well-trained as you. Large
datasets that are appended over time (think institutional data) also
have a tendency to suffer from drift or entropy. Many times
you will receive a dataset with incomplete or incomprehensible
variable labels. You will need to figure out what to do with
these. The labels in this example dataset are obviously a mess.  

```{r}
start <- 'load CA school data with problems'
end <- 'LOOKING FOR OUTLIERS WITH VARIOUS PLOTS'
writeLines(logparse(lf, start = start, end = end))
```
<br>

#### Quick Exercise
> Rename the variable labels to be appropriate and descriptive.

<br>

## Techniques for working with single variables

Each of the following describes techniques that can help you to find
data points that may be unreliable or wrong. When you find such a data
point, you need to rely on your judgment and the context of the
problem to decide what to do next.

### Outliers

Look for outliers: values that are extraordinarily far from the mean
or median (e.g., more than 3 s.d. away for approximately normal data).

Let's take a look at student teacher ratio using the `boxplot` and
`histogram` commands.

```{r}
start <- 'box plot str'
end <- 'histogram str'
writeLines(logparse(lf, start = start, end = end))
```

#### Box plot of `str`

```{r, results = 'asis'}
writeLines(alignfigure('box_str.png', 'center'))
```

```{r}
start <- 'histogram str'
end <- 'IMPOSSIBLE VALUES'
writeLines(logparse(lf, start = start, end = end))
```

#### Histogram of `str`

```{r, results = 'asis'}
writeLines(alignfigure('hist_str.png', 'center'))
```
<br>

As you can see, there are two values of student teacher ratio that are
big outliers. What should we do about these?

<br>

#### Quick Exercise
> There's another variable that has an extreme outlier. Find this
> variable and decide what to do about it. 

<br>

### Atypical values

Look for values that are atypical given the context of the student.
For instance, be vary of students in rural schools who report living
in a town of 2 million.

<br>

### Impossible values

You should also look for impossible values. These include: negative
values for things that must be positive like income or height; test
scores that are above the maximum; proportions that are negative or
above one; or percentages that are negative or above 100.

Here's a summary of the `calw_pct` variable, which is expressed in
percentage terms. It clearly has at least one impossible value.

```{r}
start <- 'summarize calw percent'
end <- 'LOOKING FOR IMPLAUSIBLE VALUES WITH VARIOUS PLOTS'
writeLines(logparse(lf, start = start, end = end))
```

<br>

#### Quick Exercise
> There's another variable that has at least one impossible
> value. Find it. 

<br>

### Data that are off trend

When looking at data that are in panel format, very sharp changes from
the previous period may be suspect. For instance, a student who goes
from the 5th percentile to the 95th percentile in test scores.

## Comparing two or more variables

### Checking relationships

Check to make sure that the variable is in the order that you would
expect in the comparison. Are there high income students that are
coded as low SES? Are there students with low GPAs and high test 
scores? These may be correct, but you need to check for strange
patterns.

To test this, let's plot several of the variables against one another
and look for problematic relationships. Here's a plot of average
income, `avginc`, against the percent of students in the district on
free or reduced price lunches, `meal_pct`:

```{r}
start <- 'twoway scatter of avginc and meal_pct'
end <- 'CHECKING CALCUATIONS'
writeLines(logparse(lf, start = start, end = end))
```

#### Average income by percentage of students on free meals

```{r, results = 'asis'}
writeLines(alignfigure('sc_inc_meal.png', 'center'))
```
<br>

#### Quick Exercise
> There's another implausible value based on the relationship between
> two variables. Find it. (Hint: what is the biggest budget item in
> any school district?)
<br>

### Logically impossible combinations

Check that there aren't logically impossible combinations of
variables. For example, we should be suspicious when a parent's age is less than that of
the student. Sometimes your dataset documentation will alert you to
potential problems. The codebook of the National Longitudinal Survey of
Youth, 1997, helpfully explains that "researchers should note that
[work hour] totals above 168 hours per week are suspect."

<br>

## Check calculations

Check to make sure that calculations have been done correctly. For
example this dataset has several ratio measures. Let's recalculate
these and see if they are correct. In the code below we recalculate
the student teacher ratio and then plot it against the original
calculation.

```{r}
start <- 'CHECKING CALCUATIONS'
end <- 'assume negative means missing and drop'
writeLines(logparse(lf, start = start, end = end))
```
#### Student teacher ratio, recalculated and plotted against old

```{r, results = 'asis'}
writeLines(alignfigure('sc_str_str_two.png', 'center'))
```

<br>

That's not good. Not good at all. `teacher` has a lot of the same -4
values. Maybe that means something. Clearly a school can't have -4
teachers. Let's recompute the ratio but only for schools with a
positive number of teachers.

```{r}
start <- 'assume negative means missing and drop'
end <- 'LOOKING FOR DUPLICATES'
writeLines(logparse(lf, start = start, end = end))
```
#### Student teacher ratio, re-recalculated and plotted against old

```{r, results = 'asis'}
writeLines(alignfigure('sc_str_str_three.png', 'center'))
```

<br>

That's a little better, but we still clearly have some problems that
we would need to deal with before a full analysis.

<br>

#### Quick Exercise
> There are other variables with problems in their calculations. Find
> them. 

<br>

## Duplicates

You'll also need to look for duplicates in variables that shouldn't
have any. The most obvious place to look is in *id* numbers or the
equivalent. You should also check any other variable that ought to be
unique. Luckily, Stata has a whole suite of duplicate commands to work
with, including, the appropriately named `duplicates`.

Below is an example of a variable with no duplicates, followed by a
variable with a couple of duplicates.

```{r}
start <- 'LOOKING FOR DUPLICATES'
end <- 'CHECK FOR NEGATIVE VALUES, MISSING DATA'
writeLines(logparse(lf, start = start, end = end))
```

<br>

## Missing data

### Find missing data codes

First, figure out how missing data is coded from the codebook. Code
those values as missing. That's a dot, `.`, in Stata.

<br>

### Recode problematic data points as missing

Next, look for impossible values as above. These should also be
recoded to missing.

<br>

### Dealing with zeros

Now, look for zeros. Are these really zeros? What was the criteria for
having a zero? Check if these should really be missing.

Your best friend here is the Stata command `inspect`. Here's the
result of the `inspect` command for two of the variables:

```{r}
start <- 'CHECK FOR NEGATIVE VALUES, MISSING DATA'
end <- 'plot reading scores for further investigation'
writeLines(logparse(lf, start = start, end = end))
```

<br>

The variable `read_scr` has a number of negative values. This warrants
futher inspection.

```{r}
start <- 'plot reading scores for further investigation'
end <- 'end file'
writeLines(logparse(lf, start = start, end = end))
```
#### Histogram of reading scores

```{r, results = 'asis'}
writeLines(alignfigure('hist_read_scr.png', 'center'))
```

<br>

#### Quick Exercise
> Look for other variables with either negative or missing values and
> figure out what do with them. Once you've got the data properly
> coded and missing entered as missing, use the `mvpatterns` command
> to figure out if there are systematic problems with your data.

<br>

## Conclusion

All of the above is only to get you started. There is no cookbook way
to approach data cleaning. The idea is to get to know the data well
enough that you know whether anomalies are just strange, but true, or
problems in the data themselves. 

<br><br>

*Init: 25 August 2015; Updated: `r format(Sys.Date(), format = "%d %B %Y")`*

<br>
