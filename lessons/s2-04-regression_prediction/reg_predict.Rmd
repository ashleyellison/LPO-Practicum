---
title: Regression Prediction
author: LPO 9952 | Spring 2018
output: github_document
---


  
```{r, echo = FALSE, message = FALSE}
require(knitr)
source('../../r/stataknitrhelper.r')
opts_chunk$set(echo = FALSE, message = FALSE, comment = NA)
```


Too often, analysts consider the analysis done when they’ve run a regression and then reported some tables. You should consider reporting your parameter estimates as the start of your report, not the end. In particular, you should think about what your results predict. The point of almost all policy anlaysis is to predict what would happen to the dependent variable if the indpendent variable changed. This is the essence of prediction.

You’ll want to use prediction for several different purposes, each of which we’ll go through.
• Toshowhowwellthemodelpredictsthedatausedtoestimateparameters • Tomakeout-ofsamplepredictionsusingtheregressionline.
• Toforecastresultsforindividualsinsample
• Toforecastresultsforindividualsoutofsample
A bit of theory
this section follows the treatment in Wooldridge.
We know that overall, the prediction is summarized in yˆ.
yˆ = βˆ 0 + βˆ 1 x 1 + βˆ 2 x 2 . . . βˆ k x k Our parameter for the prediction is θ:
θ0 =β0+β1c1+β2c2...+βkck =E(y|x1 =c1,x2 =c2...xk =ck)
The estimate of θ is therefore
θˆ = βˆ + βˆ c + βˆ c . . . βˆ c 001122kk
Of course, θ0 is not measured without error. Instead, we need to make use of the uncertainty surrounding our estimates βˆk which go into the estimate.
To accomplish this, we can plug the definition of β0 from above into the population model:
β0 =θ0−β1c1−β2c2...−βkck
y = β0 + β1 x1 + β2 x2 . . . βk xk + u =θ0−β1c1−β2c2...βkck +β1x1+β2x2...βkxk =θ0+β1(x1−c1)+β2(x2−c2)...+β2(xk −ck)
Ineffect,wesubtractthespecificvaluescj fromeachvalueofxj andregressyi ontheresult, we’ll get a set of estimates where the intercept and error term are the predicted value of y for thelinearcombinationofvaluesofxj containedinxc
Predicting data in sample
We’re using the caschool.dta data again. We’ll run two regressions, a basic one with no con- trols showing the impact of student teacher ratios on math test scores, then another again esti- mating the relationship after controlling for other characteristics of the school districts.
. /*******************************/
. /* Analysis */
. /*******************************/
.
. reg `y ́ `x ́
      Source |       SS       df
-------------+------------------------------
       Model |  5635.62443     1  5635.62443
    Residual |  141735.097   418   339.07918
-------------+------------------------------
       Total |  147370.722   419  351.720099
MS
Number of obs = 420 F( 1, 418) = 16.62 Prob>F = 0.0001 R-squared = 0.0382 Adj R-squared = 0.0359 Root MSE = 18.414
------------------------------------------------------------------------------
    math_scr |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         str |  -1.938591   .4755165    -4.08   0.000    -2.873292   -1.003889
       _cons |   691.4174   9.382469    73.69   0.000     672.9747    709.8601
------------------------------------------------------------------------------
.
. eststo  basic
.
. reg `y ́ `x ́ `controls ́
      Source |       SS       df       MS
-------------+------------------------------
       Model |  106651.228     6  17775.2047
    Residual |  40719.4931   413  98.5944143
-------------+------------------------------
       Total |  147370.722   419  351.720099
Number of obs = 420 F( 6, 413) = 180.29 Prob>F = 0.0000 R-squared = 0.7237 Adj R-squared = 0.7197 Root MSE = 9.9295
------------------------------------------------------------------------------
    math_scr |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         str |  -.2217831   .3355029    -0.66   0.509    -.8812893    .4377232
  expn_stu_t |  -.0070057   1.044094    -0.01   0.995    -2.059407    2.045395
      avginc |   .7093258   .1037914     6.83   0.000     .5053005    .9133511
2
      el_pct |  -.1097502   .0372649    -2.95   0.003    -.1830028   -.0364976
    meal_pct |  -.3824315   .0330651   -11.57   0.000    -.4474284   -.3174346
    comp_stu |   14.11309   8.116897     1.74   0.083     -1.84249    30.06868
       _cons |   663.7802   10.64377    62.36   0.000     642.8575    684.7029
------------------------------------------------------------------------------
. eststo basic_controls
.
. #delimit ;
delimiter now ;
. quietly esttab * using my_models.tex,          /* estout command: * indicate
> s all estimates in memory. csv specifies comma sep, best for excel */
>                label
> riables */
> nodepvars
>                b(2)
> s two sig digits */
> not >/
> se(2)
> nostar
> r2 (2)
> ar2 (2)
>                scalar(F  "df_m D.F. Model" "df_r D.F. Residual" N)
> ct stats from the ereturn (list) */
> sfmt (2 0 0 0)
> replace
> nomtitles >;
. #delimit cr
delimiter now cr
.
/*Use labels for models and va
/* I don ́t want t statistics *
/* sele
/* Use my model titles */
/* b= coefficients , this give
 /* I do want standard errors */
/* No stars */
/* R squared */
/* Adj R squared */
   /* format for stats*/
/* replace existing file */
What we ant to do is to first show the overall relationship between student teacher ratios and test scores and to indicate our uncertainty for the regression line. This is when prediction comes in handy.
       . // Predict using data in memory
       . predict yhat, xb
       .
       . //Get SE of prediction
       . predict yhat_se,stdp
       .
       . // Generate Prediction interval
       . gen low_ci=yhat-(`myt ́*yhat_se)
       . gen hi_ci=yhat+(`myt ́*yhat_se)
       .
       . sort `x ́
       .
       . graph twoway scatter `y ́ `x ́,msize(small) mcolor(blue)  ///
       >   || line yhat `x ́,lcolor(red) ///
       >   || line low_ci `x ́, lcolor(red) lpattern(dash) ///
       >   || line hi_ci `x ́, lcolor(red) lpattern(dash) ///
       >       legend( order(1 "Math score" 2 "Prediction" 3 "95% Confidence Interval
       > ")) ///
       >       name(basic_predict)
       .
       .
This gives us the following plot:
Remember that the prediction interval does not tell us where we can expect any individual
unit to be located. Instead, the prediction interval tells us the likely range of lines that would be 3
Table 1: OLS Results, Dependent Variable= Math (1)
Test Scores (2)
-0.22 (0.34)
-0.01 (1.04)
0.71 (0.10)
-0.11 (0.04)
-0.38 (0.03)
14.11 (8.12)
￼￼￼Student Teacher Ratio
Expenditures per Student (1000s)
Average Income
English Language Percent
Percent on Free/Reduced Meals
Computers per Student
Constant Observations
R2
Adjusted R2 F
D.F. Model D.F. Residual
Standard errors in parentheses
-1.94 (0.48)
691.42 663.78 (9.38) (10.64) 420 420
0.04 0.72 0.04 0.72
￼16.62
1 6
180.29 418 413
￼￼4
Figure 1: Predicted values of math scores across observed student teacher ratios
￼￼￼￼￼￼￼￼￼￼￼￼￼15 20 25 Hypothetical Values of Computers per Student
generated in repeated samples.
Hypothetical Values
Many times, we’d also like to think about how the dependent variable would increase or de- crease as a function of hypothetical values of x. Using only Stata’s predict command, we’re stuck with just using the data in memory. The margins command can help us to make predic- tions for hypothetical values of the independent variable.
There are two steps to using margins. First, we need to generate values of yˆ across levels of x, then we need to generate the standard error of yˆ across those same levels of x. With those estimates in hand, we can save them in memory and plot them.
       . /*Making use of the margins command*/
       .
       .
       . // Use summary to get min and max of key IV
       . sum `x ́, detail
       .
       . local mymin=r(min)
       . local mymax=r(max)
       .
       . estimates restore basic_controls
       .
       . local dfr=e(df_r)
       .
       . #delimit ;
5
Predicted Values of Math Test Scores
652 652.5 653 653.5 654 654.5
       delimiter now ;
       . margins , /* init margins */
       >     predict(xb) /* Type of prediction */
       >     nose /* Don ́t give SE */
       >     at( (mean) /* Precition at mean of all variables */
       >     `controls ́ /* Set controls at mean */
> `x ́=(`mymin ́(.1)`mymax ́)) /*range from min to max of x in steps of .1 * >/
> post /* Post results in matrix form */
>;
. #delimit cr
delimiter now cr
.
. // Pull results
. mat xb=e(b)
.
. // store x values used to generate predictions
. mat allx=e(at)
.
. // store just x values from that matrix
. matrix myx=allx[1...,1] ́
.
. // Bring back in regression results
. estimates restore basic_controls
.
. // Run margins again, but this time get standard error of prediction as outp > ut
. margins , predict(stdp) nose at(`x ́=(`mymin ́(.1)`mymax ́) (mean) `controls ́) > post
.
. //Grab standard error of prediction
. mat stdp=e(b)
.
. //Put three matrices together: standard error, prediction, and values of x: > transpose
. mat pred1=[stdp xb myx] ́
.
. //Put matrix in data
. svmat pred1
.
. //Generate
. generate lb = pred12 - (`myt ́ * pred11) /*Prediction minus t value times SE > */
. generate ub = pred12 + (`myt ́* pred11) /*Prediction plus t value times SE */ .
.
This gives us the following plot:
Forecast Intervals
Forecasting is distinct from prediction in the parlance of regression. The prediction interval is all about how different the regression line is likely to be in repeated samples. The forecast interval is all about how well the model predicts the location of individual points. A 95% confi- dence interval around the regression line says: “In 95 percent of repeated samples, an interval calculated in this way will include the true value of the regression line.” A 95% forecast interval around the regression line says “In 95 percent of repeated samples, an interval calcuated in this way will include all but 5 percent of observations.”
The process for generating these lines is very similar to the one we just went through, with the exception that we’ll be using stdf, the standrad error of the forecast, as opposed to stdp, the standard error of the prediction.
6
Figure 2: Predicted Value of Math Scores Across Hypothetical Levels of Student Teacher Ratio
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0 20 40 60 80 100 Hypothetical Values of Student-Teacher Ratio
Predicted Value ￼ ￼ Lower/Upper Bound 95% CI
￼Here’s what the forecast interval looks like for us, when predicting using available data:
With hypothetical data, we’re forecasting out of range, and so the intervals are going to be quite wide.
The point is that we should approach these results with some humility. Too often, we don’t take forecast intervals very seriously. Predictions are made on “average” using the conditional expectation function. If you’re going to forecast for an individual unit— a person, a school, a state— you need to acknowledge that the uncertainty is likely to be large.
7
Predicted Values of Math Test Scores
550 600 650 700
￼Figure 3: Predicted Value of Math Scores Across Levels of Student Teacher Ratio, Prediction vs. Forecasting
600 650 700
15 20 25 Student Teacher Ratio
Math score Prediction
95% Confidence Interval, Prediction 95% Confidence Interval, Forecast
8
Figure 4: Predicted Value of Math Scores Across Hypothetical Levels of Student Teacher Ratio, Forecast Interval
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0 20 40 60 80 100 Hypothetical Values of Student-Teacher Ratio
Predicted Value ￼ ￼ Lower/Upper Bound 95% CI
￼9
Predicted Values of Math Test Scores
550 600 650 700

