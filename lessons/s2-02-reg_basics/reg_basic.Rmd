---
title: Basics of Regression in Stata
author: LPO 9952 | Spring 2017
output: github_document
---
  
```{r, echo = FALSE, message = FALSE}
require(knitr)
source('../../r/stataknitrhelper.r')
opts_chunk$set(echo = FALSE, message = FALSE, comment = NA)
```


```{r}
## save log file in object
lf <- 'reg_basic.log'
```

```{r, engine = 'bash'}
## run accompanying .do file to get log file for parsing
#stata -b -q do ../do/lecture11_descriptives.do
## convert plots used in this file to png
#pdir=/
glist=(
      simple_scatter
      simple_lowess
       scatter_lowess
       scatter_linear
       residplot
       residplot_fancy
       
)

for i in ${glist[@]};
do
convert -density 150 -flatten $pdir$i.eps $pdir$i.png;
done
```

## Intro

Stata was made for regression. It has the most advanced suite of
regression functions and the easiest to use interface of any
statistical programming environment. This session will get you started
with how to estimate parameters for the simple regression model in
STATA.

We'll be using data from the National Longitudinal Survey of Youth,
1997. For more information about the NLSY 97 sample, click 
[here](https://www.nlsinfo.org/content/cohorts/nlsy97/intro-to-the-sample/nlsy97-sample-introduction-0). 

## Simple regression model


We'll be working with the same regression model as Wooldridge, with
$y$ as a linear function of $x$. 

$$
  y_i=\beta_o +\beta_1 x_i +u_i
$$

We're interested in coming up with estimates of the unknown population
parameters $\beta_0$ and $\beta_1$. 

Since we'll be doing OLS, we'll make all of the standard assumptions:


- The function $y_i=\beta_o +\beta_1 x_i +u$ is linear in parameters

-  Our sample, including data $y_i$ and $x_i$ has been drawn
  randomly.

- There's variation in $x$

-  The expected value of the error given the covariate is 0:
  $E(u|x)=0$, and the same is true in the sample,   $E(u_i|x_i)=0$,
  meaning that $x$ is fixed in repeated samples


The estimators $\hat{\beta_0}$, $\hat{\beta_1}$ are unbiased given the
above assumptions hold. This means that  $E(\hat{\beta_1}=\beta_1)$ in repeated
sampling. 

Let's figure out how income and postsecondary attainment are
related. Using the NLSY97
data set, we will get estimates for the following population regression model:

$$
income_i=\beta_0+\beta_1(Months_i)+u_i
$$

## Plotting Data

Before we do this, let's do a scatterplot. The scatterplot is the most
fundamental graphical tool for regression. As a starting rule, never
run a regression before looking at a scatterplot. In the accompanying
do file, I've included the macros for setting this up in terms of $x$
and $y$. 

First, let's just plot $y$ as a function of $x$:


```{r}
start <- 'First plot the data'
end <- 'Add a lowess fit'
writeLines(logparse(lf, start = start,end=end))
```


```{r, results = 'asis'}
writeLines(alignfigure('simple_scatter.png', 'center'))
```

We can than add a lowess fit to see what the shape of the relationship between $x$ and $y$ looks like. 

There are a variety of ways to check on the pattern on the data. A
lowess regression gives you a local average estimate, which is
sensitive to the patterns in the data:


```{r}
start <- 'Add a lowess fit'
end <- 'Linear fit to the data'
writeLines(logparse(lf, start = start,end=end))
```

```{r, results = 'asis'}
writeLines(alignfigure('scatter_lowess.png', 'center'))
```


*Quick Exercise: do the same with another covariate*

Our next step is to plot a linear fit to the data. 


```{r}
start <- 'Linear fit to the data'
end <- 'Get regression results'
writeLines(logparse(lf, start = start,end=end))
```


```{r, results = 'asis'}
writeLines(alignfigure('scatter_linear.png', 'center'))
```

## Estimating Regression in Stata

We start with a basic regression of income on months of postsecondary education. 
 There are a couple of ways of
describing this, one is just to say we estimate a regression
predicting income as a function of postseconcary attendance. Another way is to say we
regress income on postsecondary attendance. 


```{r}
start <-'Get regression results' 
end <- 'Extracting regression results'
writeLines(logparse(lf, start = start,end=end))
```


I'll go through each part of the above table in class. 

*Quick Exercise*

Run a regression with same dependent variable but a different
independent variable. Interpret the results in one sentence. Write
this sentence down. 

## Extracting Results

One key skill for today is being able to extract individual parts of the regression estimates from what Stata stores in memory. 
You need to build a map from the equations we'll be discussing to what can be accessed in Stata. Below, we start by extracting the regression coefficients. 

```{r}
start <-'Extracting regression results' 
end <- 'Where are the standard errors '
writeLines(logparse(lf, start = start,end=end))
```

The standard errors are stored as a variance-covariance matrix. To get a standard error, we need to take the square root of the elements of the diagonal of this matrix. 

```{r}
start <-'Where are the standard errors'
end <- 'Another way to get results back'

writeLines(logparse(lf, start = start,end=end))
```

We can use a different approach to get the same scalars. In Stata, referencing `_b[<varname>]` will pull the scalar for the coefficient asscoiated with the variable name. 
Similarly, referencing `_se[<varname>]` will get the standard error for that coefficient. 

```{r}
start <-'Another way to get results back'
end <- 'Use different confidence intervals'
writeLines(logparse(lf, start = start,end=end))
```

*Quick exercise: using both of the above methods, extract the estimate for the intercept*

## Confidence Intervals

By default, Stata gives 95% confidence intervals. To get confidence intervals at a different level, use the following code:

```{r}
start <-'Use different confidence intervals'
end <- 'How to get residual' 
writeLines(logparse(lf, start = start,end=end))
```

*Quick exercise: run the regression again, but this time get 80% CI*

## Residuals and Predictions

Residuals are not stored as part of the estimation results, but can be generated through the `predict` command. 
Below, I use the `predict` command to get residuals for this estimation. 

```{r}
start <-'How to get residual' 
end <- 'Plot residuals by x'

writeLines(logparse(lf, start = start,end=end))
```

These residuals can then be plotted as a function of $x$. 

```{r}
start <-'Plot residuals by x'
end <- 'Predictions'
writeLines(logparse(lf, start = start,end=end))
```


```{r, results = 'asis'}
writeLines(alignfigure('residplot.png', 'center'))
```

A more complex plot includes actual, predicted, and residual values. 

```{r, results = 'asis'}
writeLines(alignfigure('residplot_fancy.png', 'center'))
```

The predicted value of y is also generated via the `predict` command. 

```{r}
start <-'Predictions'
end <- 'Measures of fit'
writeLines(logparse(lf, start = start,end=end))
```

These predicted values can be plotted relative to the actual data. 

```{r, results = 'asis'}
writeLines(alignfigure('predict.png', 'center'))
```

*Quick Exercise: comple the above steps with an alternative covariate*

## Measures of Model Fit

The first measure of model fit we consider is the $F$ statistic. There are several ways to think about the $F$ statistic. For now, I'm going to suggest that you think 
of it as the ratio of two measures. The first measure is the difference between the predicted value and the mean, or how different are your predictions than what would be predicted
using the unconditional mean. The second measure is the difference between the predicted value and the actual value. We'll discuss this in class, but you should have an intuitive sense as to why 
the former should be large relative to the latter. 

```{r}
start <-'Measures of fit'
end <- 'What is R squared?'
writeLines(logparse(lf, start = start,end=end))
```

The most commonly used measure of model fit is $R^2$. It is simply the square of the correlation between the actual dv and the predicted dv. 

```{r}
start <-'What is R squared?'
end <- 'Tests of statistical significance'
writeLines(logparse(lf, start = start,end=end))
```

Below, I conduct a test of statistical significance "by hand" to show how this is done in Stata. 

```{r}
start <-'Tests of statistical significance'
end <- NULL
writeLines(logparse(lf, start = start,end=end))
```
