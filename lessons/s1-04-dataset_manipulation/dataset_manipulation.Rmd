---
title: Dataset Manipulation
author: LPO 9951 | Fall 2016
output: md_document
---
## Dataset Manipulation

```{r, echo = FALSE, message = FALSE}
require(knitr)
source('../../r/stataknitrhelper.r')
opts_chunk$set(echo = FALSE, message = FALSE, comment = NA, cache = FALSE)
```

```{r, engine = 'bash'}
## run accompanying .do file to get log file for parsing
#stata -b -q do dataset_manipulation.do	 
```

```{r}
## save log file in object
lf <-'dataset_manipulation.log'
```

<br>

#### PURPOSE

Learning to manipulate datasets is a key skill for statistical
analysis. Today we'll work on four skills: subsetting data using
preserve and restore commands, appending
data, doing a simple one-to-one merge, and collapsing data.

<br>

## Read in data

Two additional quick examples. `import delimited` can be used to read in a wide variety of delimited files. 

`import excel`, well, imports . . . excel. Take a careful look at the excel worksheet before importing to understand which range should be imported. 

```{r}
start <- 'Data import'
end <- 'set globals for url data link and local data path'
writeLines(logparse(lf, start = start, end = end))
```



For the rest of today's lecture, we'll use the `apipop` dataset, which is
available online.

```{r}
start <- 'set globals for url data link and local data path'
end <- 'split into three datasets: elementary, middle, and high school'
writeLines(logparse(lf, start = start, end = end))
```

## Subsetting with `preserve` and `restore`

A feature and sometimes curse of Stata is that it can only hold one
dataset in active memory at a time. As a feature, it helps you keep
your work organized and, through numerous warning messages, tries to
make sure you don't lose your work by accidentally forgetting to save
or mindlessly overwritting your data. The feature feels more like a
curse when you have multiple datasets that you would like to work with
simultaneously or, as we will do below, split a single dataset into
smaller parts.

To repeatedly subset a large dataset, there are two primary choices:  
  1. Reload the full dataset into memory after each subset and save  
  2. Use the `preserve` and `restore` commands

In the code below, notice how the `preserve` and `restore` commands
bookend the `keep` command, which keeps only those observations that
fulfill the `if` statement (in this case, the type of school). The
steps are:  
  1. preserve dataset in memory  
  2. subset to keep only school type that we want  
  3. save new subset dataset  
  4. restore old dataset  

```{r}
start <- '-4- restore old dataset'
end <- 'merging via the append command'
writeLines(logparse(lf, start = start, end = end))
```

<br>

## Appending Data

Appending data is done when we want to add additional
*observations* to an existing dataset, using a dataset that has
exactly the same variable names but different observations. Suppose
you have data on high schools, middle schools, and elementary schools
on a variety of performance indicators and you'd like to merge them
together. The syntax uses, appropriately enough, the `append` command,
which takes the format `append <new dataset>` (the command assumes the
first dataset is the one in memory; remember that the middle school subset data
are still in memory):

```{r}
start <- 'merging via the append command'
end <- 'merging via the merge command'
writeLines(logparse(lf, start = start, end = end))
```
The `append` command will not copy over labels from the using
dataset, so you'll need to make sure they're right in the master
dataset. The most common error with an append command is to not have
exactly matching variable names.

<br>

## Merging Data

You can also use Stata's `merge` command to do an append operation in
special cases. This happens when the merging variable doesn't have
repeated *observations* in the two datasets, which in turn have exactly
the same variable structure. Think of a Venn diagram where the circles
contain exactly the same types of information, but don't overlap; in
combining them, we've really just grown them into one bigger circle.
One of the virtues of using `merge` when `append` will suffice is that
you have access to more information about where the data came from
once you're done.

```{r}
start <- 'merging via the merge command'
end <- 'show merge stats for each merge'
writeLines(logparse(lf, start = start, end = end))
```
<br>

Once you've completed the merge, you can take a look at the \_merge_*
variables that were generated to see where the data came from.

```{r}
start <- 'show merge stats for each merge'
end <- 'split dataset by variables'
writeLines(logparse(lf, start = start, end = end))
```
<br>

#### QUICK EXERCISE
> Create a dataset that has just middle and elementary schools. Do
> this using first the `append` command and then the `merge` command.

<br>

## One-to-one merges

A one-to-one merge is when you have exactly the same *observations*
but new variables to add to the dataset. Say you have *observations*
with variables split across datasets, e.g., School 1 has variables A,
B, and C in dataset 1 and variables X, Y, and Z in dataset two. As
long as School 1 has a unique identifier---a name, an id number,
etc---you can `merge` these two datasets together so that you have
access to all of the school's variables for your analysis.

First, we need to subset our data again, only this time by splitting
along columns (*variables*) rather than rows (*observations*):

```{r}
start <- 'split dataset by variables'
end <- 'merging back together'
writeLines(logparse(lf, start = start, end = end))
```

Now for the merge and view of merge stats:

```{r}
start <- 'merging back together'
end <- 'collapsing data'
writeLines(logparse(lf, start = start, end = end))
```

<br>

#### QUICK EXERCISE
> Create a dataset that has only mobility and percent tested. Next
> create another dataset that has only the year round and percent
> responding variables. Now merge these two datasets together using a
> one-to-one merge.

<br>

## Collapsing data

Collapsing data refers to summarizing data across a type and creating
a new dataset as a result. Say we want to create a county-level
dataset from our school data, using the average figures for the
schools across a set of characteristics. The command would look like
this:

```{r}
start <- 'collapsing data'
end <- 'end file'
writeLines(logparse(lf, start = start, end = end))
```

*NB:* Notice the check using the `unique` command. If the number
 didn't align or gave some unexpected value, we should recheck our
 data and code.

<br>

#### QUICK EXERCISE
> Create a district level dataset that contains district level
> averages for the following variables:  
>   1. apioo  
>   2. api99  
>   3. ell  
>   4. meals  
> Do the same thing using just district *medians*. 

<br>
<br>

*Init: 15 August 2015; Updated: `r format(Sys.Date(), format = "%d %B %Y")`*

<br>
