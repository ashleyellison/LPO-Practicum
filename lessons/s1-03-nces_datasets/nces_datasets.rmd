---
title: NCES Datasets
author: LPO 9951 | Fall 2015
output: md_document
---

```{r, echo = FALSE, message = FALSE}
require(knitr)
source('../../r/stataknitrhelper.r')
opts_chunk$set(echo = FALSE, message = FALSE, comment = NA, cache = FALSE)
```

```{r, engine = 'bash'}
## run accompanying .do file to get log file for parsing
#stata -b -q do ../do/nces_datasets.do	 
```

```{r}
## save log file in object
lf <- 'nces_datasets.log'
```

<br>

#### PURPOSE

Today we will work on downloading data files from the NCES and OECD
databases. While these data files represent only a fraction of publicly
available data (which themselves are only a fraction of all
potentially available data), they are some of the most widely used in
education research.

<br>

## EDAT

NCES has created a very useful data tool called EDAT, which I assume
stands for something clever. The intent behind this web-based
application is to allow you to select the variables that you would
like to work with, then to generate syntax (in our case, do files)
that will allow you to access this data. Today, we'll go through how
to generate a very basic analysis dataset from all four of the
surveys that you will be working with as a group. The process for
each is broadly the same:

* Select variables
* Download syntax and data (only need to download the data the first time)
* Adjust syntax as appropriate (rewrite do-file to our specifications)
* Generate analysis dataset

This process, like all other work, should be tracked carefully. Some
simple steps at the beginning can save you lots of time at the end.

I will also show you how to download the full datasets for each survey
today. While EDAT may entirely serve your data-gathering needs (at
least for these surveys), you may in the future easier to simply
gather all of available data on your computer with a few lines of
code, avoiding the point-and-click aspects of EDAT altogether and
increasing the reproducibility of your project.

<br>

## Directory structure

First things first, let's add to our directory structure. As you
remember from the first lecture, data
files and Stata do files have been stored in their own
subdirectories. While it's possible to simply dump everything in one
big directory, you may find that over time, as the folder grows, it
becomes very difficult to find what you need and almost impossible to
share your work with others. Yes, your computer can search really
well. An organized directory structure is for you, the human. Get into
the habit now, and you'll be thankful later. 

Today we're adding an `./aux` subdirectory. Your directory structure
should now look like this: 

```
.
|-- /aux
|   |
|   |-- <auxiliary files>
|
|-- /data
|   |
|   |-- <data files>
|
|-- /do
|   |
|   |-- <Stata do files>
|
|-- /plots
|   |
|   |-- <plot files>
```

<br>

## Global variables (macros)

Now that your project/course directory is structured this way, it is
very easy to find files across subdirectories. But rather than retype
something like `../data/` in front of every data file name, it is
useful to store the relative link name in a type of variable that we can then
call as we want. One type of variable that Stata allows us to use for
this procedure (among others) is called a global variable or
macro. See the top part of the do file for an example of how to store
a relative path in a global variable:

```{r}
start <- 'set globals for entire file'
end <- 'display globals'
writeLines(logparse(lf, start = start, end = end))
```

Note that globals follow the pattern: `global <name> <value>`.
To call a global macro in Stata, you place a `$` in front of the name
you gave it. Stata will replace that with the value. Here are some
examples with `display`:

```{r}
start <- 'display globals'
end <- 'Educational'
writeLines(logparse(lf, start = start, end = end))
```

#### NOTE: Most calls of the Stata global macro do not require quotation marks. That is a quirk of Stata's `display` command. 

<br>

## Educational Longitudinal Study, 2002 (ELS)

<br>

### Unzip data

Prior to running this file, we will have to have downloaded the
entirety of the ELS student file. While this may
seem like overkill, note that even if you use EDAT and subset the
variables you actually want, you will end up downloading all of ELS
anyway (just the way it works). First, let's set new globals:

```{r}
start <- 'set globals for ELS files'
end <- 'unzip ELS file'
writeLines(logparse(lf, start = start, end = end))
```

As you probably noticed, the downloaded ELS file is zipped. This means
that we need to unzip it with `unzipfile`. Stata will only unzip a
file into the current directory, so in order to have it go into our
data directory like we want, we need to use the `cd` command to change
directory into our data directory and return to the working directory
after we've unzipped the file. This is why we saved the `workdir` at
the top of the file. 

```{r}
start <- 'unzip ELS file'
end <- 'change delimiter to a semi-colon -1-'
writeLines(logparse(lf, start = start, end = end))
```

<br>

### Subset data

ELS data files are very large, so they take up a ton of memory if you
try to load them in their entirety. For any given project, you don't
want or need all of the variables anyway. To subset the full dataset
to only those variables we want requires the `use using` setup, as
follows (note how we temporariliy change the delimiter to make our
lives a little easier):

```{r}
start <- 'change delimiter to a semi-colon -1-'
end <- 'Early Childhood'
writeLines(logparse(lf, start = start, end = end))
```

Because nobody (nobody) likes variables with capital letters---except
in very specific sitations---we use the `renvars` command to set all
the variable names to lowercase. Once that is finished, we save the
new working dataset with `save` command. By prepending the name of the
saved dataset with the relative link to the data folder (via the
global), the save dataset goes into the correct subdirectory. The
option after the comma, `replace`, simply tells Stata to overwrite any
files with same name.

<br>

#### QUICK EXERCISE
> Using EDAT, generate an ELS dataset that includes student demographics
> and whether or not the student attended college.

<br>

## Early Childhood Longitudinal Study - Kindergarten 98-99 (ECLS-K)

<br>

### Unzip data

Unzipping ECLS-K follows the same method that we used for ELS. Note
that the unzipped files is *really* big, so sure you have enough room
for it (no 2 GB thumbdrives!). 

```{r}
start <- 'Early Childhood Longitudinal Study - Kindergarten'
end <- 'download dictionary file that we will need'
writeLines(logparse(lf, start = start, end = end))
```
<br>

### Subset data

Also like the ELS files, the ECLS-K data files are too large to load
all at once. Unlike ELS, however, they require a separate dictionary file to
properly read in the data. When downloading from EDAT, you will also
get a `.dct` file that you will use to properly parse the `.dat`
format of ECLS-K. It will look something like this:

```
dictionary {

_column(1)     str8 CHILDID   %8s       "CHILD IDENTIFICATION NUMBER"
_column(1418)  double C3R4RSCL%6.2f     "C3 RC4 READING IRT SCALE SCORE"
_column(1510)  double C3R4MSCL%6.2f     "C3 RC4 MATH IRT SCALE SCORE"

}
```

I've already created this file and hosted it in the course
repository. Since it is a small file, we can use the `copy` command,
which takes the format `copy <from> <to>` and can handle URL paths, to
download the file to the auxiliary directory. Note again the use of
global macros:

```{r}
start <- 'download dictionary file that we will need'
end <- 'read in ECLS file'
writeLines(logparse(lf, start = start, end = end))
```

To read the ECLS-K `.dat` file, we use the `infile` command. Note the
weird double `using` subcommands. The first tells Stata which
dictionary file to use; the second, after the comma, tells which
dataset to use. There are a couple of other ways to use this command,
but this is the most transparent. As before, we lower variable names
and save the working dataset.

```{r}
start <- 'read in ECLS file'
end <- 'High School Longitudinal Study'
writeLines(logparse(lf, start = start, end = end))
```
<br>

#### QUICK EXERCISE
> Using EDAT, generate an ECLS-K dataset that includes student
> demographics and student IRT math scores from the first three waves.

<br>

## High School Longitudinal Study, 2009 (HSLS)

<br>

### Unzip and subset data

The process for unzipping and subsetting data from HSLS
is the same as for ELS. In interest of completeness, the code is
reproduced in full below:

```{r}
start <- 'High School Longitudinal Study'
end <- 'Programme for International Student Assessment'
writeLines(logparse(lf, start = start, end = end))
```
<br>

## Programme for International Student Assessment (PISA)

<br>

### Unzip data

Not all datasets, of course, come from the NCES. One major dataset,
PISA, is conducted and hosted by the Organization for Economic
Co-operation and Development or OECD. To get these data, we'll once
again use the same process for unzipping files. 

```{r}
start <- 'Programme for International Student Assessment'
end <- 'store variables'
writeLines(logparse(lf, start = start, end = end))
```
<br>

### Subset data

PISA uses a fixed-width format for storing data that is common but
not particularly user friendly. The first 3 lines of text look like this (they
are long so they wrap in this view):

```
ALB0080000ALB00060000800000000100001 777777777790100177237002777114393137900772077777777771077777777777772077777709r100777777777777777777777777777777777777777777777777777777777777777777777777773777002277771117773103777777777777232141777777777779797979997979797979997777799941232121121199972222222999712777777777777777777777777777777777777777777771127772117777777777777212121127777777777714028NOV13
ALB0080000ALB00060000800000000100002 97077777777077737727710270777977773791177777719977777777777777777777777773397777771907777777777777777777777711777111999993311111918877734077103099127777777777777777777777777777777777777777777777777777777777797979797979797979799977777777214312211211 1007777777999777211112777711111199117777777771112211177777777777777777777777777777777777777777777777714028NOV13
ALB0080000ALB00060000800000000100003 39777777777177717747710470777377773701919777777777233774211777777777777777777777777777777777777777777777710477024777777777777777777711B77710777777771110217771B37777802877777777777777777773127777777771092212197979797979797019799977777777421322111112
```

To read the data into Stata, we have to use the `infix` command, which
requires that we know the column of each variable that we need. It
also helps if we know the data format (e.g., string vs. integer
vs. float,
etc.). [Having a codebook like this is absolutely necessary](http://pisa2012.acer.edu.au/downloads/M_stu_codebook.pdf).
Knowing what is needed, we can finally read in the data and save a
reduced dataset. Because the string is long, I've stored the entire
thing in a local variable using the comand `local <name> ...`. Locals work just
like globals except they only last for as long as the script is
running. To call a local, surround the local name by left and right
ticks. Note, that a left tick is different from a right tick. On most
keyboards, the left tick is above the tab key on the lefthand side of the keyboard.

```{r}
start <- 'store variables'
end <- 'end file'      
writeLines(logparse(lf, start = start, end = end))
```
<br>

#### QUICK EXERCISE
> Following the link above, figure out the column for the variable
> about the number of computers in the household, add it to the list,
> and read in the new expanded data.

<br>
<br>

*Init: 03 August 2015; Updated: `r format(Sys.Date(), format = "%d %B %Y")`*

<br>
